import torch
import torch.nn as nn
import torch.nn.functional as F

class Encoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Encoder, self).__init__()
        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)
        self.fc_mu = nn.Linear(128, latent_dim)
        self.fc_logvar = nn.Linear(128, latent_dim)

    def forward(self, x):
        _, (h, _) = self.lstm(x)
        h = h.squeeze(0)  # remove the sequence dimension
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

class Decoder(nn.Module):
    def __init__(self, latent_dim, output_dim):
        super(Decoder, self).__init__()
        self.latent_dim = latent_dim
        self.output_dim = output_dim
        self.lstm = nn.LSTM(latent_dim, 128, batch_first=True)
        self.fc = nn.Linear(128, output_dim)

    def forward(self, z, seq_len):
        z = z.unsqueeze(1).repeat(1, seq_len, 1)  # replicate z along sequence dimension
        x, _ = self.lstm(z)
        x = self.fc(x)
        return x

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        self.encoder = Encoder(input_dim, latent_dim)
        self.decoder = Decoder(latent_dim, input_dim)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decoder(z, x.size(1))  # x.size(1) is the sequence length
        return recon, mu, logvar

# Example usage:
# Assuming you have a DataLoader set up for your motion capture dataset
# Define your training loop
def train_vae(vae, train_loader, num_epochs=10, learning_rate=1e-3):
    optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)

    def loss_function(recon_x, x, mu, logvar):
        # Reconstruction loss (mean squared error)
        MSE = F.mse_loss(recon_x, x, reduction='sum')
        # KL divergence
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return MSE + KLD

    vae.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_idx, data in enumerate(train_loader):
            optimizer.zero_grad()
            recon_batch, mu, logvar = vae(data)
            loss = loss_function(recon_batch, data, mu, logvar)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')

# Example usage:
# Assuming train_loader is your DataLoader for motion capture data (amc files)
# and your_input_dim and your_latent_dim are appropriately defined
vae = VAE(input_dim=your_input_dim, latent_dim=your_latent_dim)
train_vae(vae, train_loader)
